<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Predicting NYTimes Comment Success by CS109-comment</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Predicting NYTimes Comment Success</h1>
      <h2 class="project-tagline">Andrew Petschek, Jonathan Friedman, Reinier Maat</h2>
      <a href="https://github.com/CS109-comment/NYTimes-Comment-Popularity-Prediction" class="btn">View on GitHub</a>
    </section>

    <section class="main-content">
      <p align="center">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/mFjHJQVsTa8" frameborder="0" allowfullscreen></iframe>
      </p>

<h2>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h2>

<p>As one of the most popular online news entities, The New York Times (NYT) <a href="http://www.journalism.org/media-indicators/digital-top-50-online-news-entities-2015/">attracts thousands</a> of unique visitors each day to its website, <a href="http://www.nytimes.com">nytimes.com</a>. Users who visit the site can provide their thoughts and reactions to published content by posting comments. </p>

<p>The website receives around 9,000 submitted comments per day, over 60,000 unique contributors per month, and approximately two million comment recommendations (i.e., "likes") <a href="http://www.nytimes.com/2015/11/23/insider/the-most-popular-reader-comments-on-the-times.html">each month</a>. There is a dedicated moderation staff committed to review each submission one-by-one and even hand-select the very best comments as "NYT Picks."</p>

<p>The Times embraces this personal, intimate approach to comment moderation based on the hypothesis that "readers of The Times would demand an elevated experience." Click <a href="http://www.nytimes.com/2015/11/23/insider/the-most-popular-reader-comments-on-the-times.html">here</a> to learn more about the staff at the NYT who work on this.</p>

<h2>
<a id="our-angle" class="anchor" href="#our-angle" aria-hidden="true"><span class="octicon octicon-link"></span></a>Our Angle</h2>

<p>We have examined the relationship between comment success (i.e., the number of recommendations it receives by other users and if it is selected as a NYT Pick) as well as other features about the comments themselves. Specifically, we have built a model that can predict the success of a given comment. We envision this model as a complementary tool that could be used by the moderators during their daily review of comments. For example, perhaps there is a comment they are unsure about; they could run our model to see the comment's predicted success.</p>

<p>This tool could also benefit the commenters themselves. Perhaps this model could help a commenter predict how their draft comment would perform once submitted. An effective prediction system could also be used in an automated comment recommender to help steer users toward higher quality content. </p>

<p>The Times cares about its comments, as is evident in a recent <a href="http://www.nytimes.com/interactive/2015/11/23/nytnow/23commenters.html?_r=0">piece</a> published where their top 14 commenters were identified and interviewed. </p>

<p align="center">
    <img src="TopCommenters.png" width="600">
</p>

<h2>
<a id="comments-comments-comments" class="anchor" href="#comments-comments-comments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Comments, Comments, Comments</h2>

<p>The NYT website receives nearly a quarter million comment submissions a month. After analyzing a set of 180,000 comments spanning 2 years, we were able to learn some interesting statistics about all these comments. </p>

<h3>
<a id="most-people-only-comment-once" class="anchor" href="#most-people-only-comment-once" aria-hidden="true"><span class="octicon octicon-link"></span></a>Most people only comment once</h3>

<p>Assuming commenters do not use multiple accounts, the statistics indicate that most comments come from users who have never posted before. In other words, since the median of our dataset is 1, this implies the majority of <strong>users who post, only post once</strong>. The mean, however, is 4, which is due to a few outliers included in the overall calculation. </p>

<h3>
<a id="majority-of-comments-receive-5-recommendations" class="anchor" href="#majority-of-comments-receive-5-recommendations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Majority of comments receive &lt;5 recommendations</h3>

<p>As it turns out, most comments receive fewer than five recommendations. However, there are examples of highly recommended comments; we found a comment with 3,064 recommendations.</p>

<h3>
<a id="its-hard-to-be-a-nyt-pick" class="anchor" href="#its-hard-to-be-a-nyt-pick" aria-hidden="true"><span class="octicon octicon-link"></span></a>It's hard to be a NYT Pick</h3>

<p>Out of our 180,000 comments, we found less than 3% to be designated as Editor's Pick. Of those picks, we see that they get more recommendations that those that are not picks. Specifically, NYT Picks comments receive an average of 180 recommendations. A high correlation between recommendation count and NYT Pick would not be surprising, but it is an interesting statistic nonetheless. </p>

<p align="center">
    <img src="NYTPicks.png" width="600">
</p>

<h2>
<a id="the-data" class="anchor" href="#the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Data</h2>

<h3>
<a id="scraping" class="anchor" href="#scraping" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scraping</h3>

<p>We obtained the comment data from The New York Times <a href="http://developer.nytimes.com/docs">API</a>. The API operates similarly to the Huffington Post API that was used earlier in the course.</p>

<h3>
<a id="transformation" class="anchor" href="#transformation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformation</h3>

<p>We transformed the recommendation count of all comments into two classes, 'high' and 'low', to make prediction easier. This meant that all comments with less than 16 recommendations were considered 'low' (the 75th percentile) and all above that were classified as 'high'.  </p>

<h2>
<a id="features" class="anchor" href="#features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Features</h2>

<h4>
<a id="bag-of-words" class="anchor" href="#bag-of-words" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bag of words</h4>

<p>We use a binary bag of words feature that encodes which of the 200 most popular words appear in a comment.</p>

<h4>
<a id="sentiment" class="anchor" href="#sentiment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sentiment</h4>

<p>We used sentiment analysis to extract a positive and negative sentiment score for every comment. We hypothesized that comment recommendations may depend on sentiment, as posts with a strong sentiment are likely to be more controversial than neutral posts.</p>

<h4>
<a id="tf-idf" class="anchor" href="#tf-idf" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tf-idf</h4>

<p>Intuitively, tf-idf measures how important a word is in a document compared with how important the word is to the corpus as a whole. Words that appear frequently in documents but appear rarely in the corpus receive high scores.</p>

<h4>
<a id="avg-word-length" class="anchor" href="#avg-word-length" aria-hidden="true"><span class="octicon octicon-link"></span></a>Avg. word length</h4>

<p>Longer, more sophisticated word usage may correlate with better comments.</p>

<h4>
<a id="word-count" class="anchor" href="#word-count" aria-hidden="true"><span class="octicon octicon-link"></span></a>Word count</h4>

<p>The word count may be indicative of the quality of a comment.</p>

<h2>
<a id="the-model" class="anchor" href="#the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Model</h2>

<p>First, we tried a few models, linear regression and random forest regression, on the recommendation count data. When we found out that these models performed poorly, we switched to binary classification of high recommendation and low recommendation posts at a cutoff value of the 75th percentile (16 recommendations). For this classification problem, we again tried multiple models, among which were linear SVC and the random forest classification. The random forest classifier performed moderately well, but the rest still performed quite poorly.</p>

<h2>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2>

<p>Our random forest classifier performed okay, with a precision of 0.98, and a recall of 0.75. The accuracy, at 0.75, was unfortunately not higher than a baseline 'low' predictor.</p>

<p>Here's a summary of our results:</p>

<p align="center">
    <img src="confusion.png">
</p>

<p>As you can see, we are dealing with a substantial amount of false negatives. This means it is hard for the model to mark actual good posts as good. Identifying bad comments, ones that have a low recommendation count, on the other hand, is easy.</p>

<h2>
<a id="discussion" class="anchor" href="#discussion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discussion</h2>

<p>We had a great deal of trouble generating an even moderately useful prediction. There were two main reasons for this: (1) highly unbalanced data, and (2) the difficulty of natural language processing.</p>

<ul>
<li><p><strong>Unbalanced data:</strong> As we showed in exploratory data analysis section, the vast majority of comments comments have very few recommendations, and only a small proportion of comments are designated editor's choices. This results in a dataset where predicting zero recommendations and editor's choices is effective at minimizing error. It is, in general, hard to make any sort of good predictions when the data is this unbalanced. One straightforward, but time-consuming, approach to ameliorating this problem is to get more data. This would likely be the first step in a future analysis. 180,000 comments is only a small proportion of the total comments posted each year, and collecting more data would give us more popular comments on which to train our models.</p></li>
<li><p><strong>NLP:</strong> NLP is a deep and complicated field, and since we did not have prior experience, we were able to perform only rudimentary feature selection. Given more time, we could research and implement more sophisticated feature selection techniques and engineer features that carry more information about the comments.</p></li>
</ul>

<p>We could further improve our model by exploring how an article relates to its comments. As a simple example, positive sentiment sentiment comments on restaurant reviews might fare better than positive comments on highly politicized editorials. A model could derive more complex relationships. With a larger sample of comments and article data, we could use a deep learning approach to derive insights from the complicated relationships between articles and comments, and between comments and other comments. Building a model that incorporates article text and metadata could be very powerful; unfortunately, it would also require much more data scraping and much more sophisticated methods, both of which are time-prohibitive.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/CS109-comment/NYTimes-Comment-Popularity-Prediction">Predicting NYTimes Comment Success</a> is maintained by <a href="https://github.com/CS109-comment">CS109-comment</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
