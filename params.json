{"name":"Predicting NYTimes Comment Success","tagline":"Andrew Petschek, Jonathan Friedman, Reinier Maat","body":"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mFjHJQVsTa8\" frameborder=\"0\" allowfullscreen></iframe>\r\n\r\n## Background\r\nAs one of the most popular online news entities, The New York Times (NYT) [attracts thousands](http://www.journalism.org/media-indicators/digital-top-50-online-news-entities-2015/) of unique visitors each day to its website, [nytimes.com](http://www.nytimes.com). Users who visit the site can provide their thoughts and reactions to published content by posting comments. \r\n\r\nThe website receives around 9,000 submitted comments per day, over 60,000 unique contributors per month, and approximately two million comment recommendations (i.e., \"likes\") [each month](http://www.nytimes.com/2015/11/23/insider/the-most-popular-reader-comments-on-the-times.html). There is a dedicated staff committed to review each submission one-by-one and even hand-select the very best comments as \"NYT Picks.\"\r\n\r\nThe Times embraces this personal, intimate approach to comment moderation based on the hypothesis that \"readers of The Times would demand an elevated experience.\" Click [here](http://www.nytimes.com/2015/11/23/insider/the-most-popular-reader-comments-on-the-times.html) to learn more about the staff at the NYT who work on this.\r\n\r\n## Our Angle\r\nWe have examined the relationship between comment success (i.e., the number of recommendations it receives by other users and if it is selected as a NYT Pick) as well as other features about the comments themselves. Specifically, we have built a model that can predict the success of a given comment. We envision this model as a complementary tool that could be used by the moderators during their daily review of comments. For example, perhaps there is a comment they are unsure about; they could run our model to see the comment's predicted success.\r\n\r\nThis tool could also benefit the commentor's themselves. Perhaps this model could help a commenter predict how their draft comment would perform once submitted. An effective prediction system could also be used in an automated comment recommender to help steer users toward higher quality content. \r\n\r\nThe Times cares about its comments, as is evident in a recent [piece](http://www.nytimes.com/interactive/2015/11/23/nytnow/23commenters.html?_r=0) published where their top 14 commenters were identified and interviewed. \r\n\r\n<img src=\"TopCommenters.png\" width=\"600\">\r\n\r\n## Comments, Comments, Comments\r\nThe NYT website receives nearly a quarter million comment submissions a month. After analyzing a set of 180,000 comments spanning 2 years, we were able to learn some interesting statistics about all these comments. \r\n\r\n### Most people only comment once\r\n\r\nAssuming commenters do not use multiple accounts, the statistics indicate that most comments come from users who have never posted before. In other words, **users who post, only post once**. \r\n\r\n### Majority of comments receive <5 recommendations\r\nAs it turns out, most comments receive fewer than five recommendations. However, there are examples of highly recommended comments. The maximum we found was 3064 recommendations, which makes sense why we see a mean of ..\r\n\r\n### It's hard to be a NYT Pick\r\nOut of our 180,000 comments, we found less than 3% to be designated as Editor's Pick. Of those picks, we see that they get more recommendations that those that are not picks. Specifically, NYT Picks comments receive an average of 180 recommendations. We suspect there is a high correlation between recommendation count and NYT Pick, but it is an interesting statistic nonetheless. \r\n\r\n<img src=\"NYTPicks.png\" width=\"600\">\r\n\r\n## The Data\r\n\r\n### Scraping\r\nWe obtained the comment data from The New York Times [API](http://developer.nytimes.com/docs). The API operates similarly to the Huffington Post API that was used earlier in the course.\r\n\r\n### Transformation\r\nWe transformed the recommendation count of all comments into two classes, 'high' and 'low', to make prediction easier. This meant that all comments with less than 16 recommendations were considered 'low' (the 75th percentile) and all above that were classified 'high'.  \r\n\r\n## Features\r\n#### Bag of words\r\nWe use a binary bag of words feature that encodes which of the *n* most popular words appear in a comment.\r\n\r\n#### Sentiment\r\nWe used sentiment analysis to extract a positive and negative sentiment score for every comment. We hypothesized that comment recommendations may depend on sentiment, as posts with a strong sentiment are likely to be more controversial than neutral posts.\r\n\r\n#### Tf-idf\r\nIntuitively, tf-idf measures how important a word is in a document compared with how important the word is to the corpus as a whole. Words that appear frequently in documents but appear rarely in the corpus receive high scores.\r\n\r\n#### Avg. word length\r\nLonger, more sophisticated word usage may correlate with better comments.\r\n\r\n#### Word count\r\nThe word count may be indicative of the quality of a comment.\r\n\r\n## The Model\r\nWe tried several models of which only one had a reasonably good performance, which was the Random Forest Classifier.\r\n\r\n## Results\r\nOur model performed reasonably, with a precision of 0.98, and a recall of 0.75. The accuracy, at 0.75, was not higher than a baseline 'low' predictor.\r\n\r\nHere is the confusion matrix:\r\n\r\n\r\n## Discussion\r\nWe had a great deal of trouble generating an even moderately useful prediction. There were two main reasons for this: (1) highly unbalanced data, and (2) the difficulty of natural language processing.\r\n\r\n* **Unbalanced data:** As we showed in exploratory data analysis section, the vast majority of comments comments have very few recommendations, and only a small proportion of comments are designated editor's choices. This results in a dataset where predicting zero recommendations and editor's choices is effective at minimizing error. It is, in general, hard to make any sort of good predictions when the data is this unbalanced. One straightforward, but time-consuming, approach to ameliorating this problem is to get more data. This would likely be the first step in a future analysis. 180,000 comments is only a small proportion of the total comments posted each year, and collecting more data would give us more popular comments on which to train our models.\r\n\r\n* **NLP:** NLP is a deep and complicated field, and since we did not have prior experience, we were able to perform only rudimentary feature selection. Given more time, we could research and implement more sophisticated feature selection techniques and engineer features that carry more information about the comments.\r\n\r\nWe could further improve our model by exploring how an article relates to its comments. As a simple example, positive sentiment sentiment comments on restaurant reviews might fare better than positive comments on highly politicized editorials. A model could derive more complex relationships. With a larger sample of comments and article data, we could use a deep learning approach to derive insights from the complicated relationships between articles and comments, and between comments and other comments. Building a model that incorporates article text and metadata could be very powerful; unfortunately, it would also require much more data scraping and much more sophisticated methods, both of which are time-prohibitive.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}